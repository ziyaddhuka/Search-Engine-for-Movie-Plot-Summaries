{"cells":[{"cell_type":"code","source":["sc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"704a60aa-45cf-456f-aad5-8aa3c8b1f468"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=847800122948729#setting/sparkui/0625-190152-spoke958/driver-5427525271009083303\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=847800122948729#setting/sparkui/0625-190152-spoke958/driver-5427525271009083303\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["%sh \npip install nltk\npip install --upgrade pip\npython -m nltk.downloader all"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0a76e94-deea-4f38-91dd-3937f4749b08"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.6.2)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (from nltk) (2021.4.4)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.0.1)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.61.1)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (0.17.0)\nRequirement already satisfied: pip in /databricks/python3/lib/python3.8/site-packages (21.1.2)\n/usr/local/lib/python3.8/runpy.py:127: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading collection &#39;all&#39;\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /root/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /root/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package brown to /root/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package comparative_sentences is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /root/nltk_data...\n[nltk_data]    |   Package dolch is already up-to-date!\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package floresta to /root/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v15 is already up-to-date!\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v17 is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to /root/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /root/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /root/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /root/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /root/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to /root/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /root/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /root/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pil to /root/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package propbank to /root/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package ptb to /root/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package qc to /root/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to /root/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rte to /root/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package semcor to /root/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to /root/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to /root/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package state_union to /root/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package timit to /root/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to /root/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /root/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n[nltk_data]    |   Package verbnet3 is already up-to-date!\n[nltk_data]    | Downloading package webtext to /root/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /root/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | Downloading package rslp to /root/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /root/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n[nltk_data]    |       up-to-date!\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package perluniprops is already up-to-date!\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n[nltk_data]    |   Package wmt15_eval is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.6.2)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (from nltk) (2021.4.4)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.0.1)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.61.1)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (0.17.0)\nRequirement already satisfied: pip in /databricks/python3/lib/python3.8/site-packages (21.1.2)\n/usr/local/lib/python3.8/runpy.py:127: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading collection &#39;all&#39;\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /root/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /root/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package brown to /root/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package comparative_sentences is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /root/nltk_data...\n[nltk_data]    |   Package dolch is already up-to-date!\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package floresta to /root/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v15 is already up-to-date!\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v17 is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to /root/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /root/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /root/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /root/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /root/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to /root/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /root/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /root/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pil to /root/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package propbank to /root/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package ptb to /root/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package qc to /root/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to /root/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rte to /root/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package semcor to /root/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to /root/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to /root/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package state_union to /root/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package timit to /root/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to /root/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /root/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n[nltk_data]    |   Package verbnet3 is already up-to-date!\n[nltk_data]    | Downloading package webtext to /root/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /root/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | Downloading package rslp to /root/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /root/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n[nltk_data]    |       up-to-date!\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package perluniprops is already up-to-date!\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n[nltk_data]    |   Package wmt15_eval is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from nltk.corpus import stopwords\nimport re\nimport string\nimport math"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f8cbad9-9b08-4cb7-98d0-b59bf7e4d05c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["movie_metadata = sc.textFile('/FileStore/tables/movie_metadata.tsv')\nplot_summaries = sc.textFile('/FileStore/tables/plot_summaries.txt')\n\n# Loading the stopwords document from nltk library\nstop_words = set(stopwords.words(\"english\"))\n\n# https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n# plot_summaries_punct.collect() would remove the punctuation from the input text\nplot_summaries_punct = plot_summaries.map(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n\n# plot_summaries_stop_wds.collect() would output (movie_id, summary) with stopwords removed from the summary\nplot_summaries_stop_wds = plot_summaries_punct.map(lambda x: (x.split('\\t')[0],' '.join([w for w in x.split('\\t')[1].split(' ') if not w.lower() in stop_words])))\n\n# below two plot_summaries.collect() would give ((movie_id,word),1)\nplot_summaries_mapped = plot_summaries_stop_wds.flatMap(lambda x: (((x[0],word.lower()),1) for word in x[1].split(' ')))\nplot_summaries_mapped = plot_summaries_mapped.filter(lambda x: x[0][1]!='')        # removing empty '' words from the data\n\n# term_frequency.collect() would output ((movie_id, word),count_of_word_in_that_movie_summary)\nterm_frequency = plot_summaries_mapped.reduceByKey(lambda x,y : x+y)\n\n# calculating document frequency for a word, intermediate_step would have (word,1) and when we reduce it by key we will get (word,document_frequency_of_the_word)\nintermediate_step = term_frequency.map(lambda x: (x[0][1],1))\ndocument_frequency = intermediate_step.reduceByKey(lambda x,y: x+y)\n\n# steps to count total number of documents using mapReduce\n# we take the given input file data and map it to (1,1) and then reduce it to get total number of documents\ncount_doc = plot_summaries.map(lambda x:(1,1))\ntotal_doc = count_doc.reduceByKey(lambda x,y: x+y).collect()\ntotal_doc = total_doc[0][1]\n\n# This step will calculate log(N/df) output would be (word, log(N/df)_value)\nlog_of_N_by_document_freq = document_frequency.map(lambda x: (x[0],math.log(total_doc/x[1])))\n\n# we map term_frequency correctly so that we could join it with log_of_N_by_document_freq\n# term_frequency_for_join.collect() would output (word, (movie_id, count_of_word_in_that_movie_summary))\nterm_frequency_for_join = term_frequency.map(lambda x: (x[0][1],(x[0][0],x[1])))\n\n# joining the term_frequency_for_join and log_of_N_by_document_freq which gives us (word, ((movie_id,count_of_word_in_that_movie_summary), log(N/df)_value))\ntf_idf = term_frequency_for_join.join(log_of_N_by_document_freq)\n\n# now we need to multiply count_of_word_in_that_movie_summary and log(N/df)_value to get tfidf values\n# at last we have output as ((movie_id, word), tfidf_for_that_word)\ntf_idf_values = tf_idf.map(lambda x: ((x[1][0][0],x[0]),x[1][0][1]*x[1][1])).collect()\n\ntf_idf_data = sc.parallelize(tf_idf_values)\n\n# to answer query of a single word we will need movie_metadata to extract movie names from the movie_id\nmovie_id_and_name = movie_metadata.map(lambda x: (x.split('\\t')[0],x.split('\\t')[2]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fc76973-ff37-4afa-8b25-9d0839ad9c8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3abb7fe-3e2d-42fc-89d9-9b807c829f0d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Input word, can be changed as per needs\nword_input = \"scifi\"\n\n# would check if words in movie_summary match with the input word and filter out those not matching\n# query_filter would contain ((movie_id, word), tfidf_for_that_word)\nquery_filter = tf_idf_data.filter(lambda x: x[0][1].lower()==word_input)\n\n# Mapping query_filter to get just the movie_id and tfidf_for_that_word and join it with movie_id_and_name to get movie names \n# sorting that to get only top 10 tfidf values and then mapping that output to just get the movie name\nquery_filter.map(lambda x: (x[0][0],x[1])).join(movie_id_and_name).sortBy(lambda x: -x[1][0]).map(lambda x: x[1][1]).take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Query- Single word Input","showTitle":true,"inputWidgets":{},"nuid":"ba23685c-bbc5-4262-8e75-30dcfc08c3a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: [&#39;The Animatrix&#39;,\n &#39;Dance of the Dead&#39;,\n &#39;On Your Mark&#39;,\n &#39;The Arcadian&#39;,\n &#39;Boldly Going Nowhere&#39;,\n &#39;Arthur Christmas&#39;,\n &#39;Red Cockroaches&#39;,\n &#39;Music Machine&#39;,\n &#39;Bowfinger&#39;,\n &#39;Mutant Swinger from Mars&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [&#39;The Animatrix&#39;,\n &#39;Dance of the Dead&#39;,\n &#39;On Your Mark&#39;,\n &#39;The Arcadian&#39;,\n &#39;Boldly Going Nowhere&#39;,\n &#39;Arthur Christmas&#39;,\n &#39;Red Cockroaches&#39;,\n &#39;Music Machine&#39;,\n &#39;Bowfinger&#39;,\n &#39;Mutant Swinger from Mars&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16539032-6d7c-46d9-9c70-63abca17e77c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Input query, can be changed as per need\nsentence_query = \"A murder mystery weekend\"\n\n# removing punctuation from the query if any\nsentence_query = sentence_query.translate(str.maketrans('', '', string.punctuation))\n\n# removing the stopwords from the query\nsentence_query = [word for word in sentence_query.lower().split(\" \") if not word in stop_words]\n\n# converting the sentence to words\nsentence_query_rdd = sc.parallelize(sentence_query)\nsentence_query_rdd = sentence_query_rdd.filter(lambda x: x!='')\n\nwords_to_search = sentence_query_rdd.collect()\n\n# getting inverse document frequency of the query: output would be ((word, count_of_word_in_query), log(N/df)_value)\nquery_idf = sentence_query_rdd.map(lambda x: (x,1)).reduceByKey(lambda x,y : x+y).join(log_of_N_by_document_freq)\n\n# getting the term frequency for the query words, output would be ((word, term_frequency_of_that_word), 1)\nquery_tf = sentence_query_rdd.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n\n# joining the term frequency and log_of_N_by_document_freq and mapping to get tfidf for the query words\nquery_tfidf = query_tf.join(query_idf).map(lambda x: (x[0],x[1][0]*x[1][1][1])).reduceByKey(lambda x,y: x+y)\n\n# output (movie_id, sum_of_suqares_of_tfidf)\nsquare_of_document_tfidf = tf_idf_data.map(lambda x:(x[0][0],x[1]*x[1])).reduceByKey(lambda x,y: x + y)\n\n# output sum_of_squares_of_tfidf of query document\nsquare_of_query_tfidf = query_tfidf.map(lambda x:(x[1]*x[1])).reduce(lambda x,y: x + y) \n\n# filters the words which are included in the query\ndata_for_cosine_similarity = tf_idf_data.filter(lambda x: x[0][1].lower() in words_to_search)\n\n# mapping the filtered data to (word, (movie_id, tfidf_value))\ndata_for_cosine_similarity = data_for_cosine_similarity.map(lambda x: (x[0][1],(x[0][0],x[1])))\n\n# joining the data_fodata_for_cosine_similarity with the query_tfidf on basis of words so as to calculate the cosine similarity\n# output would be (word, ((movie_id, tfidf_value_of_that_word), tfidf_value_of_that_word_in_query))\nfinal_data = data_for_cosine_similarity.join(query_tfidf)\n\n# mapping final_data to (movie_id, (tfidf_value_of_that_word, tfidf_value_of_that_word_in_query))\nfinal_data = final_data.map(lambda x: (x[1][0][0], (x[1][0][1], x[1][1])))\n\n# here we get the data from which we can calculate the cosine similarity\n# output of the map would be (movie_id, (tfidf_value_of_that_word, tfidf_value_of_that_word_in_query), sum_of_squares_of_tfidf_of_that_document)\nfinal_data = final_data.join(square_of_document_tfidf)\n\n# applies the cosine similarity formula on the data, output is (movie_id, cosine_similarity_of_query_with_document) \nintermediate_result = final_data.map(lambda x: (x[0], (x[1][0][0]*x[1][0][1])/(math.sqrt(x[1][1])*math.sqrt(square_of_query_tfidf))))\ncosine_similarity = intermediate_result.reduceByKey(lambda x,y: x+y)\n\ntop_10_documents = cosine_similarity.sortBy(lambda x: -x[1])\n\n# output represents (Movie Name, cosine similarity between the query and document)\ntop_10_documents.join(movie_id_and_name).sortBy(lambda x: -x[1][0]).map(lambda x:(x[1][1],x[1][0])).take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Query- Sentence input","showTitle":true,"inputWidgets":{},"nuid":"1f0bec23-2c98-4313-949c-4272c4a23d2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: [(&#39;Final Stab&#39;, 0.4276294306309519),\n (&#39;Accused&#39;, 0.34855783535405593),\n (&#39;Fever Lake&#39;, 0.3445540980189404),\n (&#39;Warm Nights on a Slow Moving Train&#39;, 0.3435768845354579),\n (&#39;Under the Lighthouse Dancing&#39;, 0.2595278037222739),\n (&#39;Three on a Meathook&#39;, 0.2415228631788836),\n (&#39;Long Weekend&#39;, 0.23057231825628186),\n (&#39;The Myth of the American Sleepover&#39;, 0.21377379022008952),\n (&#39;Love in a Goldfish Bowl&#39;, 0.21043930374733083),\n (&#39;The Shadow&#39;, 0.2061580970111319)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [(&#39;Final Stab&#39;, 0.4276294306309519),\n (&#39;Accused&#39;, 0.34855783535405593),\n (&#39;Fever Lake&#39;, 0.3445540980189404),\n (&#39;Warm Nights on a Slow Moving Train&#39;, 0.3435768845354579),\n (&#39;Under the Lighthouse Dancing&#39;, 0.2595278037222739),\n (&#39;Three on a Meathook&#39;, 0.2415228631788836),\n (&#39;Long Weekend&#39;, 0.23057231825628186),\n (&#39;The Myth of the American Sleepover&#39;, 0.21377379022008952),\n (&#39;Love in a Goldfish Bowl&#39;, 0.21043930374733083),\n (&#39;The Shadow&#39;, 0.2061580970111319)]</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"part2_tfidf","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2711947211723007}},"nbformat":4,"nbformat_minor":0}
